{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from imgaug import augmenters as iaa\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get video details\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    out_filename = 'output_video.avi'\n",
    "    out = cv2.VideoWriter(out_filename, cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB for Mediapipe processing\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces and get landmarks\n",
    "        results = face_mesh.process(image_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Extract landmarks\n",
    "                landmarks = []\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "                \n",
    "                # Extract HOG features (if needed)\n",
    "                g = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray =cv2.resize(g, (48, 48))\n",
    "                hog_features = extract_hog_features(gray)\n",
    "                \n",
    "                # Ensure HOG features are flattened\n",
    "                hog_features = hog_features.flatten()\n",
    "                \n",
    "                # Concatenate HOG features and landmarks\n",
    "                features = np.hstack([hog_features, landmarks])\n",
    "                \n",
    "                # Check if feature size matches expected by SVM model\n",
    "                assert len(features) == 2304, f\"Expected 2304 features, but got {len(features)} features.\"\n",
    "                \n",
    "                # Predict using SVM model\n",
    "                predicted_label = svm_model.predict([features])[0]\n",
    "                \n",
    "                # Draw bounding box around face\n",
    "                xmin, ymin, xmax, ymax = get_bounding_box(face_landmarks, frame_width, frame_height)\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                \n",
    "                # Write label on the bounding box\n",
    "                cv2.putText(frame, predicted_label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        \n",
    "        # Write the frame into the file 'output_video.avi'\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Emotion Prediction', frame)\n",
    "        \n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Replace 'video_path' with your video file path\n",
    "video_path = r'D:\\ITI\\Graduation proj\\Videos_data\\output_video_79.mp4'\n",
    "process_video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
